# -*- coding: utf-8 -*-
"""Dataset_combined.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19UQTbKPSQ4Ps1mdp8zXVFL66KpPv4YXe
"""

import requests
from datetime import datetime, timedelta, date
import json
import os
import csv
from copy import deepcopy
import pandas as pd
from bs4 import BeautifulSoup

#**************************************************************************carbon intensity****************************************************************
today = date.today()
d = today - timedelta(days=1)
url = 'https://api.carbonintensity.org.uk/regional/intensity/%s/%s/regionid/13' %(d, today)
r = requests.get(url, allow_redirects=True)
open('carbon_intensity.json', 'wb').write(r.content)

from copy import deepcopy
import pandas


def cross_join(left, right):
    new_rows = [] if right else left
    for left_row in left:
        for right_row in right:
            temp_row = deepcopy(left_row)
            for key, value in right_row.items():
                temp_row[key] = value
            new_rows.append(deepcopy(temp_row))
    return new_rows


def flatten_list(data):
    for elem in data:
        if isinstance(elem, list):
            yield from flatten_list(elem)
        else:
            yield elem


def json_to_dataframe(data_in):
    def flatten_json(data, prev_heading=''):
        if isinstance(data, dict):
            rows = [{}]
            for key, value in data.items():
                rows = cross_join(rows, flatten_json(value, prev_heading + '.' + key))
        elif isinstance(data, list):
            rows = []
            for i in range(len(data)):
                [rows.append(elem) for elem in flatten_list(flatten_json(data[i], prev_heading))]
        else:
            rows = [{prev_heading[1:]: data}]
        return rows

    return pandas.DataFrame(flatten_json(data_in))

with open(os.getcwd() + r'/carbon_intensity.json') as json_file:
    json_data = json.load(json_file)

df = json_to_dataframe(json_data)
df.drop(['data.regionid', 'data.dnoregion', 'data.shortname', 'data.data.intensity.index'], inplace=True, axis=1)
#, 'data.data.generationmix.fuel','data.data.generationmix.perc'
df.rename(columns = {'data.data.from':'From', 'data.data.to':'To', 'data.data.intensity.forecast':'Intensity','data.data.generationmix.fuel':'Fuel','data.data.generationmix.perc':'Value'}, inplace = True)
df.to_csv('carbon_intensity1.csv', mode='w', index=False)
data = pd.read_csv(os.getcwd() + r'/carbon_intensity1.csv')
for i in range(data.shape[0]): 
    if i%9 != 0:
                 data = data.drop([i], axis = 0)
pd.to_datetime(df['To'])
data.to_csv('carbon_intensity.csv', encoding='utf-8', index=False)
os.remove(os.getcwd() + r'/carbon_intensity.json')
os.remove(os.getcwd() + r'/carbon_intensity1.csv')


#*******************************************solar***************************************************
today = date.today()
d1 = today - timedelta(days=1)
url = 'https://api0.solar.sheffield.ac.uk/pvlive/v2?start=%sT00:00:00&end=%sT23:00:00&data_format=csv' %(d1,d1)
r = requests.get(url, allow_redirects=True)
open('solar_pre.csv', 'wb').write(r.content)
data=pd.read_csv(os.getcwd() + r'/solar_pre.csv') 
for i in range(data.shape[0]): 
    if i%2 != 0:
                 data = data.drop([i], axis = 0)
data.to_csv('d1.csv', encoding='utf-8', index=False)
Data = pd.read_csv(os.getcwd() + r'/d1.csv', parse_dates=['datetime_gmt'], infer_datetime_format=True)
Data['Date1'] = Data.datetime_gmt.dt.date
Data['Time1'] = Data.datetime_gmt.dt.time
Data.drop(['datetime_gmt'], inplace=True, axis=1)
Data.to_csv('solar.csv', mode='w', index=False)
data = pd.read_csv(os.getcwd() + r'/solar.csv')
os.remove(os.getcwd() + r'/solar_pre.csv')
os.remove(os.getcwd() + r'/d1.csv')

#********************************************wind***********************************************************

today = date.today()
d2 = today - timedelta(days=1)
url = 'https://api.bmreports.com/BMRS/B1440/v1?APIKey=3pyyl4iymgrn812&SettlementDate=%s&Period=*&ServiceType=xml' %(d2)
r = requests.get(url, allow_redirects=True)
open('wind.xml', 'wb').write(r.content)


# Open XML file
file = open(os.getcwd() + r'/wind.xml', 'r')
  
# Read the contents of that file
contents = file.read()
  
soup = BeautifulSoup(contents, 'xml')
  
# Extracting the data
businessType = soup.find_all('businessType')
powerSystemResourceType = soup.find_all('powerSystemResourceType')
settlementDate = soup.find_all('settlementDate')
settlementPeriod = soup.find_all('settlementPeriod')
quantity = soup.find_all('quantity')

data = []

# Loop to store the data in a list named 'data'
for i in range(0, len(quantity)):
    rows = [ businessType[i].get_text(), powerSystemResourceType[i].get_text(), settlementDate[i].get_text(), settlementPeriod[i].get_text(), quantity[i].get_text()]
    data.append(rows)
  
# Converting the list into dataframe
df = pd.DataFrame(data, columns=['businessType', 'powerSystemResourceType', 'settlementDate' , 'settlementPeriod', 'quantity'
                                 'settlementDate' ], dtype = float)
df.rename(columns={ "powerSystemResourceType" : "wind_type", "businessType" : "type", "settlementDate": "date", "settlementPeriod": "period", "quantity" : "quantity" }, inplace = True)
df.to_csv('wind.csv', index=False)
df = pd.read_csv(os.getcwd() + r'/wind.csv')
df = df.replace(['\"','\"'], ['',''], regex=True)
df.to_csv('wind.csv', index=False)
data=pd.read_csv(os.getcwd() + r'/wind.csv') 
for i in range(data.shape[0]): 
    if i%3 == 0:
                 data = data.drop([i], axis = 0)
data.to_csv('wind1.csv', encoding='utf-8', index=False)
data=pd.read_csv(os.getcwd() + r'/wind1.csv', nrows=96) 
for i in range(data.shape[0]): 
    if i%2 == 0:
                 data = data.drop([i], axis = 0)
data.to_csv('wind_onshore.csv', encoding='utf-8', index=False)
data=pd.read_csv(os.getcwd() + r'/wind1.csv', nrows=96) 
for i in range(data.shape[0]): 
    if i%2 != 0:
                 data = data.drop([i], axis = 0)
data.to_csv('wind_offshore.csv', encoding='utf-8', index=False)

os.remove(os.getcwd() + r'/wind.csv')
os.remove(os.getcwd() + r'/wind1.csv')
os.remove(os.getcwd() + r'/wind.xml')

#*********************************Weather********************************************

url = 'http://api.weatherapi.com/v1/forecast.json?key=c300eafa74ce4f6486f75424211806&q=London&days=1&aqi=no&alerts=no'
r = requests.get(url, allow_redirects=True)
open('weather.json', 'wb').write(r.content)

def cross_join(left, right):
    new_rows = [] if right else left
    for left_row in left:
        for right_row in right:
            temp_row = deepcopy(left_row)
            for key, value in right_row.items():
                temp_row[key] = value
            new_rows.append(deepcopy(temp_row))
    return new_rows


def flatten_list(data):
    for elem in data:
        if isinstance(elem, list):
            yield from flatten_list(elem)
        else:
            yield elem


def json_to_dataframe(data_in):
    def flatten_json(data, prev_heading=''):
        if isinstance(data, dict):
            rows = [{}]
            for key, value in data.items():
                rows = cross_join(rows, flatten_json(value, prev_heading + '.' + key))
        elif isinstance(data, list):
            rows = []
            for i in range(len(data)):
                [rows.append(elem) for elem in flatten_list(flatten_json(data[i], prev_heading))]
        else:
            rows = [{prev_heading[1:]: data}]
        return rows

    return pandas.DataFrame(flatten_json(data_in))

with open(os.getcwd() + r'/weather.json') as json_file:
    json_data = json.load(json_file)

df = json_to_dataframe(json_data)
df.drop(['location.name', 'location.country', 'location.region', 'location.lat', 'location.lon','location.tz_id', 'location.localtime_epoch', 'location.localtime', 'current.uv'], inplace=True, axis=1)
df.rename(columns = {'forecast.forecastday.hour.time':'Date_Time', 'forecast.forecastday.hour.temp_c':'Temperature'}, inplace = True)
df.to_csv('test.csv', mode='w', index=False)
Data = pd.read_csv(os.getcwd() + r'/test.csv', parse_dates=['Date_Time'], infer_datetime_format=True)
Data['Date'] = Data.Date_Time.dt.date
Data['Time'] = Data.Date_Time.dt.time
Data.drop(['Date_Time'], inplace=True, axis=1)
Data.to_csv('weather.csv', mode='w', index=False)

os.remove(os.getcwd() + r'/test.csv')
os.remove(os.getcwd() + r'/weather.json')


#*******************************************Demand_elexon***************************************


url = ('https://api.bmreports.com/BMRS/ROLSYSDEM/v1?APIKey=3pyyl4iymgrn812&ServiceType=xml') 

r = requests.get(url, allow_redirects=True)
open('demand_elexon.xml', 'wb').write(r.content)

# Open XML file
file = open(os.getcwd() + r'/demand_elexon.xml', 'r')
  
# Read the contents of that file
contents = file.read()
  
soup = BeautifulSoup(contents, 'xml')
  
# Extracting the data
settDate = soup.find_all('settDate')
publishingPeriodCommencingTime = soup.find_all('publishingPeriodCommencingTime')
fuelTypeGeneration = soup.find_all('fuelTypeGeneration')

data = []
  
# Loop to store the data in a list named 'data'
for i in range(0, len(fuelTypeGeneration)):
    rows = [ settDate[i].get_text(), publishingPeriodCommencingTime[i].get_text(), fuelTypeGeneration[i].get_text()]
    data.append(rows)
  
# Converting the list into dataframe
df = pd.DataFrame(data, columns=['settDate', 'publishingPeriodCommencingTime', 
                                 'fuelTypeGeneration' ], dtype = float)
df.rename(columns={ "publishingPeriodCommencingTime" : "Time", "settDate" : "Date", "fuelTypeGeneration": "Demand" }, inplace = True)
df['Time'] = df['Time'].apply(lambda x: x[:5])
df.to_csv('demand_elexon.csv', index=False)

os.remove(os.getcwd() + r'/demand_elexon.xml')



#*********************************************************Frequency_elexon***********************************

url = 'https://api.bmreports.com/BMRS/FREQ/v1?APIKey=3pyyl4iymgrn812&ServiceType=xml'
r = requests.get(url, allow_redirects=True)
open('freq_elexon.xml', 'wb').write(r.content)


# Open XML file
file = open(os.getcwd() + r'/freq_elexon.xml', 'r')
  
# Read the contents of that file
contents = file.read()
  
soup = BeautifulSoup(contents, 'xml')
  
# Extracting the data
reportSnapshotTime = soup.find_all('reportSnapshotTime')
spotTime = soup.find_all('spotTime')
frequency = soup.find_all('frequency')

data = []
  
# Loop to store the data in a list named 'data'
for i in range(0, len(frequency)):
    rows = [ reportSnapshotTime[i].get_text(), spotTime[i].get_text(), frequency[i].get_text()]
    data.append(rows)
  
# Converting the list into dataframe
df = pd.DataFrame(data, columns=['reportSnapshotTime', 'spotTime', 
                                 'frequency' ], dtype = float)
df.rename(columns={ "reportSnapshotTime" : "date", "spotTime" : "time", "frequency": "frequency" }, inplace = True)

df.to_csv('freq_elexon.csv', index=False)

os.remove(os.getcwd() + r'/freq_elexon.xml')


#******************************************************Energy Transmitted ****************************************************

today = date.today()
d = today - timedelta(days=30)

url = ('https://api.bmreports.com/BMRS/DEVINDOD/v1?APIKey=3pyyl4iymgrn812&FromDate=%s&ToDate=%s&ServiceType=xml') %(d,today)
r = requests.get(url, allow_redirects=True)
open('transmit_elexon.xml', 'wb').write(r.content)

file = open(os.getcwd() + r'/transmit_elexon.xml', 'r')
  
# Read the contents of that file
contents = file.read()
  
soup = BeautifulSoup(contents, 'xml')
  
# Extracting the data
settlementDay = soup.find_all('settlementDay')
volume = soup.find_all('volume')


data = []
  
# Loop to store the data in a list named 'data'
for i in range(0, len(volume)):
    rows = [ settlementDay[i].get_text(), volume[i].get_text() ]
    data.append(rows)
  
# Converting the list into dataframe
df = pd.DataFrame(data, columns=['settlementDay', 'volume'], dtype = float)
df.rename(columns={ "settlementDay" : "date", "volume" : "volume" }, inplace = True)

df.to_csv('transmit_elexon.csv', index=False)

os.remove(os.getcwd() + r'/transmit_elexon.xml')


#*******************************************************Generation fuel************************************

url = ('https://api.bmreports.com/BMRS/FUELINST/v1?APIKey=3pyyl4iymgrn812&ServiceType=xml') 


r = requests.get(url, allow_redirects=True)
open('fuel_elexon.xml', 'wb').write(r.content)


file = open(os.getcwd() + r'/fuel_elexon.xml', 'r')
  
# Read the contents of that file
contents = file.read()
  
soup = BeautifulSoup(contents, 'xml')
  
# Extracting the data
publishingPeriodCommencingTime = soup.find_all('publishingPeriodCommencingTime')
ccgt = soup.find_all('ccgt')
coal = soup.find_all('coal')
nuclear = soup.find_all('nuclear')
npshyd = soup.find_all('npshyd') #hydro
other = soup.find_all('other')

data = []
  
# Loop to store the data in a list named 'data'
for i in range(0, len(other)):
    rows = [ publishingPeriodCommencingTime[i].get_text(), ccgt[i].get_text(), coal[i].get_text(), nuclear[i].get_text(), npshyd[i].get_text(), other[i].get_text() ]
    data.append(rows)
  
# Converting the list into dataframe
df = pd.DataFrame(data, columns=['publishingPeriodCommencingTime', 'ccgt', 'coal', 'nuclear', 'npshyd', 'other'], dtype = float)
df.rename(columns={ "publishingPeriodCommencingTime" : "date", "ccgt" : "gas turbine", "npshyd" : "hydro" }, inplace = True)
df.to_csv('fuel_elexon.csv', index=False)

os.remove(os.getcwd() + r'/fuel_elexon.xml')


#**************************************************Pricing Data**********************************************
today = date.today()
d = today - timedelta(days=1)
url = ('https://api.bmreports.com/BMRS/DERSYSDATA/v1?APIKey=3pyyl4iymgrn812&FromSettlementDate=%s&ToSettlementDate=%s&SettlementPeriod=*&ServiceType=xml') %(d,today)
r = requests.get(url, allow_redirects=True)
open('price_elexon.xml', 'wb').write(r.content)

# Open XML file
file = open(os.getcwd() + r'/price_elexon.xml', 'r')
  
# Read the contents of that file
contents = file.read()
  
soup = BeautifulSoup(contents, 'xml')
  
# Extracting the data
settlementDate = soup.find_all('settlementDate')
settlementPeriod = soup.find_all('settlementPeriod')
systemSellPrice = soup.find_all('systemSellPrice')

data = []
  
# Loop to store the data in a list named 'data'
for i in range(0, len(systemSellPrice)):
    rows = [ settlementDate[i].get_text(), settlementPeriod[i].get_text(), systemSellPrice[i].get_text()]
    data.append(rows)
  
# Converting the list into dataframe
df = pd.DataFrame(data, columns=['settlementDate', 'settlementPeriod', 'systemSellPrice' ], dtype = float)
df.rename(columns={ "settlementDate" : "date", "settlementPeriod" : "period", "systemSellPrice": "price" }, inplace = True)

df.to_csv('price_elexon.csv', index=False)

os.remove(os.getcwd() + r'/price_elexon.xml')